{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:06:02.713046Z",
     "start_time": "2024-10-25T08:06:02.036871Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "import os\n",
    "import getpass"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gordontveito-duncan/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:06:16.313489Z",
     "start_time": "2024-10-25T08:06:02.713649Z"
    }
   },
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter in your OpenAI API Key:\")\n",
    "\n",
    "# Loader = PyPDFLoader(\"data/principles_of_marketing_book.pdf\")Â # If the file was local\n",
    "loader = PyPDFLoader(\n",
    "    \"https://storage.googleapis.com/strapi_cms_assets/principles_of_marketing_book.pdf\"\n",
    ")\n",
    "raw_documents = loader.load_and_split()\n",
    "print(raw_documents[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Principles of Mark eting' metadata={'source': 'https://storage.googleapis.com/strapi_cms_assets/principles_of_marketing_book.pdf', 'page': 0}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:06:16.317697Z",
     "start_time": "2024-10-25T08:06:16.314669Z"
    }
   },
   "source": [
    "len(raw_documents)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:06:16.319809Z",
     "start_time": "2024-10-25T08:06:16.318353Z"
    }
   },
   "source": [
    "# Let's just use the first 100 documents for this example:\n",
    "raw_documents = raw_documents[:100]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:06:20.494999Z",
     "start_time": "2024-10-25T08:06:16.320252Z"
    }
   },
   "source": [
    "# Load the marketing principles .pdf, split it into chunks, embed each chunk and load it into the vector store.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "# You can use OpenSource embeddings instead of OpenAIEmbeddings --> embeddings = HuggingFaceEmbeddings()\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(documents, embeddings)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:06:20.497909Z",
     "start_time": "2024-10-25T08:06:20.495768Z"
    }
   },
   "source": [
    "db"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x117c9f8b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:06:20.731721Z",
     "start_time": "2024-10-25T08:06:20.498547Z"
    }
   },
   "source": [
    "query = \"What is the license used within the principles of marketing book?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principles of Marketing by University of Minnesota is licensed under a Creative Commons Attribution-NonCommer cial-Shar eAlike 4.0\n",
      "International License , except wher e otherwise noted.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple QA retrieval system using FAISS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining similarity search across a vector database with a chat model, you can easily create a very simple question answering (QA) system. Expanding on the GPT best practices to avoid hallucinations by asking the chat model to only answer using reference text from the database, you can create a simple system that can answer questions about a specific topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:06:23.787162Z",
     "start_time": "2024-10-25T08:06:20.732502Z"
    }
   },
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "query = \"License used within the principles of marketing book\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# Combine all of the docs into a single string:\n",
    "combined_docs = \" \".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Check the length of the combined docs:\n",
    "print(len(combined_docs))\n",
    "\n",
    "# Create the chat model:\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "template = \"\"\"Given the following text, answer the following question.\n",
    "        Question: {question}\n",
    "        \n",
    "        You must follow the following principles:\n",
    "        - You must only answer using the reference text provided.\n",
    "        - If you don't the answer without the reference text, you must return \"I don't know\".\n",
    "        - It is vital that you return the answer with the reference text and not without.\n",
    "\n",
    "        Reference Text: {combined_docs}\"\"\"\n",
    "\n",
    "# Make the template:\n",
    "system_message = SystemMessagePromptTemplate.from_template(template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message])\n",
    "\n",
    "# Create the messages:\n",
    "query = \"What is the license used within the principles of marketing book?\"\n",
    "messages = chat_prompt.format_prompt(\n",
    "    question=query, combined_docs=combined_docs\n",
    ").to_messages()\n",
    "\n",
    "# Call the chat model:\n",
    "response = chat(messages)\n",
    "print(response.content)\n",
    "\n",
    "# Alternatively if we try something that the LLM doesn't have access too within the .pdf marketing book:\n",
    "query = \"What is data science?\"\n",
    "messages = chat_prompt.format_prompt(\n",
    "    question=query, combined_docs=combined_docs\n",
    ").to_messages()\n",
    "\n",
    "# Call the chat model:\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gordontveito-duncan/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The license used within the Principles of Marketing book is a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n",
      "Data science is a field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible write the above code in a simpler way with _a chain_. For now think of a chain as a _multiple steps that are executed to accomplish a task_. Additionally you will use the vector database as your back end for searching results, commonly referred to as a _retriever._\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:06:23.802965Z",
     "start_time": "2024-10-25T08:06:23.796833Z"
    }
   },
   "source": [
    "db.as_retriever()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x117c9f8b0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:06:27.119614Z",
     "start_time": "2024-10-25T08:06:23.804388Z"
    }
   },
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    chain_type=\"map_reduce\",\n",
    "    retriever=db.as_retriever(),\n",
    "    return_source_documents=True,\n",
    ")\n",
    "#qa.invoke({\"query\": \"What is book's title?\"})\n",
    "\n",
    "qa.invoke({\"query\": \"What is book's title?\"})"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not a mapping",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 11\u001B[0m\n\u001B[1;32m      3\u001B[0m qa \u001B[38;5;241m=\u001B[39m RetrievalQA\u001B[38;5;241m.\u001B[39mfrom_chain_type(\n\u001B[1;32m      4\u001B[0m     llm\u001B[38;5;241m=\u001B[39mchat,\n\u001B[1;32m      5\u001B[0m     chain_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmap_reduce\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      6\u001B[0m     retriever\u001B[38;5;241m=\u001B[39mdb\u001B[38;5;241m.\u001B[39mas_retriever(),\n\u001B[1;32m      7\u001B[0m     return_source_documents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#qa.invoke({\"query\": \"What is book's title?\"})\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[43mqa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquery\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWhat is book\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43ms title?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/chains/base.py:163\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    162\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    164\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/chains/base.py:153\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[1;32m    152\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 153\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    155\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    156\u001B[0m     )\n\u001B[1;32m    158\u001B[0m     final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    159\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[1;32m    160\u001B[0m     )\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/chains/retrieval_qa/base.py:144\u001B[0m, in \u001B[0;36mBaseRetrievalQA._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    143\u001B[0m     docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_docs(question)  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 144\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcombine_documents_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_documents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_run_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_source_documents:\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_key: answer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_documents\u001B[39m\u001B[38;5;124m\"\u001B[39m: docs}\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:148\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    146\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    147\u001B[0m     emit_warning()\n\u001B[0;32m--> 148\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/chains/base.py:574\u001B[0m, in \u001B[0;36mChain.run\u001B[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(args[\u001B[38;5;241m0\u001B[39m], callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[1;32m    570\u001B[0m         _output_key\n\u001B[1;32m    571\u001B[0m     ]\n\u001B[1;32m    573\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[0;32m--> 574\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[1;32m    575\u001B[0m         _output_key\n\u001B[1;32m    576\u001B[0m     ]\n\u001B[1;32m    578\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[1;32m    579\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    580\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    581\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m but none were provided.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    582\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:148\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    146\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    147\u001B[0m     emit_warning()\n\u001B[0;32m--> 148\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/chains/base.py:378\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[1;32m    347\u001B[0m \n\u001B[1;32m    348\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    371\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    372\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[1;32m    373\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    375\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[1;32m    376\u001B[0m }\n\u001B[0;32m--> 378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/chains/base.py:163\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    162\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    164\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/chains/base.py:153\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[1;32m    152\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 153\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    155\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    156\u001B[0m     )\n\u001B[1;32m    158\u001B[0m     final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    159\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[1;32m    160\u001B[0m     )\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/chains/combine_documents/base.py:137\u001B[0m, in \u001B[0;36mBaseCombineDocumentsChain._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001B[39;00m\n\u001B[1;32m    136\u001B[0m other_keys \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_key}\n\u001B[0;32m--> 137\u001B[0m output, extra_return_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcombine_docs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_run_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mother_keys\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m extra_return_dict[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_key] \u001B[38;5;241m=\u001B[39m output\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m extra_return_dict\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/chains/combine_documents/map_reduce.py:226\u001B[0m, in \u001B[0;36mMapReduceDocumentsChain.combine_docs\u001B[0;34m(self, docs, token_max, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcombine_docs\u001B[39m(\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    216\u001B[0m     docs: List[Document],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    220\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mdict\u001B[39m]:\n\u001B[1;32m    221\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Combine documents in a map reduce manner.\u001B[39;00m\n\u001B[1;32m    222\u001B[0m \n\u001B[1;32m    223\u001B[0m \u001B[38;5;124;03m    Combine by mapping first chain over all documents, then reducing the results.\u001B[39;00m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;124;03m    This reducing can be done recursively if needed (if there are many documents).\u001B[39;00m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 226\u001B[0m     map_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    227\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# FYI - this is parallelized and so it is fast.\u001B[39;49;00m\n\u001B[1;32m    228\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdocument_variable_name\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpage_content\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m}\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m     question_result_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_chain\u001B[38;5;241m.\u001B[39moutput_key\n\u001B[1;32m    232\u001B[0m     result_docs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    233\u001B[0m         Document(page_content\u001B[38;5;241m=\u001B[39mr[question_result_key], metadata\u001B[38;5;241m=\u001B[39mdocs[i]\u001B[38;5;241m.\u001B[39mmetadata)\n\u001B[1;32m    234\u001B[0m         \u001B[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001B[39;00m\n\u001B[1;32m    235\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i, r \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(map_results)\n\u001B[1;32m    236\u001B[0m     ]\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/chains/llm.py:227\u001B[0m, in \u001B[0;36mLLMChain.apply\u001B[0;34m(self, input_list, callbacks)\u001B[0m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    226\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 227\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    228\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)\n\u001B[1;32m    229\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutputs\u001B[39m\u001B[38;5;124m\"\u001B[39m: outputs})\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/chains/llm.py:224\u001B[0m, in \u001B[0;36mLLMChain.apply\u001B[0;34m(self, input_list, callbacks)\u001B[0m\n\u001B[1;32m    219\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[1;32m    220\u001B[0m     dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    221\u001B[0m     {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_list\u001B[39m\u001B[38;5;124m\"\u001B[39m: input_list},\n\u001B[1;32m    222\u001B[0m )\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 224\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    226\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/chains/llm.py:115\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[0;34m(self, input_list, run_manager)\u001B[0m\n\u001B[1;32m    113\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m run_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, BaseLanguageModel):\n\u001B[0;32m--> 115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    122\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mbind(stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs)\u001B[38;5;241m.\u001B[39mbatch(\n\u001B[1;32m    123\u001B[0m         cast(List, prompts), {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks}\n\u001B[1;32m    124\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:560\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    552\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    553\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    554\u001B[0m     prompts: List[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    557\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    558\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    559\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 560\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:426\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    421\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    422\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    423\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[1;32m    424\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    425\u001B[0m ]\n\u001B[0;32m--> 426\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_combine_llm_outputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mres\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_output\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mres\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mresults\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    427\u001B[0m generations \u001B[38;5;241m=\u001B[39m [res\u001B[38;5;241m.\u001B[39mgenerations \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results]\n\u001B[1;32m    428\u001B[0m output \u001B[38;5;241m=\u001B[39m LLMResult(generations\u001B[38;5;241m=\u001B[39mgenerations, llm_output\u001B[38;5;241m=\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_openai/chat_models/base.py:485\u001B[0m, in \u001B[0;36mChatOpenAI._combine_llm_outputs\u001B[0;34m(self, llm_outputs)\u001B[0m\n\u001B[1;32m    483\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m token_usage\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    484\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m overall_token_usage:\n\u001B[0;32m--> 485\u001B[0m         overall_token_usage[k] \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moverall_token_usage[k], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mv}\n\u001B[1;32m    486\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    487\u001B[0m         overall_token_usage[k] \u001B[38;5;241m=\u001B[39m v\n",
      "\u001B[0;31mTypeError\u001B[0m: 'int' object is not a mapping"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more information about Document QA retrieval here https://python.langchain.com/docs/modules/chains/additional/question_answering.html.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Similarity Search with score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS-specific techniques exist, one of which is termed as `.similarity_search_with_score`. This particular method permits the retrieval of not only the corresponding documents but also the distance measure between the query and the said documents. The returned distance measure utilizes L2 distance, where a smaller score is a better match.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:07:07.476344Z",
     "start_time": "2024-10-25T08:07:07.095541Z"
    }
   },
   "source": [
    "query = \"What is the license used within the principles of marketing book?\"\n",
    "docs_and_scores = db.similarity_search_with_score(query=query)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:07:08.094989Z",
     "start_time": "2024-10-25T08:07:08.090427Z"
    }
   },
   "source": [
    "docs_and_scores"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Principles of Marketing by University of Minnesota is licensed under a Creative Commons Attribution-NonCommer cial-Shar eAlike 4.0\\nInternational License , except wher e otherwise noted.', metadata={'source': 'https://storage.googleapis.com/strapi_cms_assets/principles_of_marketing_book.pdf', 'page': 2}),\n",
       "  0.22802575),\n",
       " (Document(page_content='Publisher Information\\nPrinciples of Marketing is adapted\\nfrom a work produced and distributed\\nunder a Creative Commons license\\n(CC BY -NC-SA) in 2010 by a\\npublisher who has requested that they\\nand the original author not receive\\nattribution. This adapted edition is\\nproduced by the University of Minnesota Libraries Publishing through the eLearning Support Initiative.\\nThis adaptation has reformatted the original text, and replaced some images and figures to make the resulting\\nwhole more shareable. This adaptation has not significantly altered or updated the original 2010 text. This work\\nis made available under the terms of a Creative Commons Attribution-NonCommercial-ShareAlike license .\\nFor questions about this textbook please contact textbookuse@umn.edu\\nix', metadata={'source': 'https://storage.googleapis.com/strapi_cms_assets/principles_of_marketing_book.pdf', 'page': 8}),\n",
       "  0.23621719),\n",
       " (Document(page_content='Principles of Mark eting\\n[Author remo ved atrequest oforiginal publisher]\\nUNIVERSITY OF MINNESO TALIBRARIES PUBLISHING EDITION, 2015. THIS EDITION AD APTED\\nFROM AWORK ORIGINALL YPRODUCED IN 2010 BY APUBLISHER WHO HAS REQUESTED THA TIT\\nNO TRECEIVE ATTRIBUTION.\\nMINNEAPOLIS, MN', metadata={'source': 'https://storage.googleapis.com/strapi_cms_assets/principles_of_marketing_book.pdf', 'page': 1}),\n",
       "  0.3636674),\n",
       " (Document(page_content='Principles of Mark eting', metadata={'source': 'https://storage.googleapis.com/strapi_cms_assets/principles_of_marketing_book.pdf', 'page': 0}),\n",
       "  0.38380933)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to add documents to the FAISS vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:07:10.542730Z",
     "start_time": "2024-10-25T08:07:10.537007Z"
    }
   },
   "source": [
    "new_documents = [\n",
    "    Document(\n",
    "        page_content=\"Data engineering begins with data collection. Often, data streams in from multiple sources, such as customer interactions, website activity, social media, IoT devices, and more.\",\n",
    "        metadata={\"title\": \"Data Engineering Book\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Pandas is a popular open-source Python library widely used for data manipulation and analysis. It provides powerful data structures like DataFrames and Series, which allow users to handle and analyze structured data easily. \",\n",
    "        metadata={\"title\": \"Pandas Analysis Book\"},\n",
    "    ),\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:07:12.241229Z",
     "start_time": "2024-10-25T08:07:11.254680Z"
    }
   },
   "source": [
    "db.add_documents(documents=new_documents)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['57caa512-c062-4a38-81e4-c9ccd8de2190',\n",
       " 'fea85409-c439-43a1-aa63-e6f485d90a5d']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:07:13.109208Z",
     "start_time": "2024-10-25T08:07:12.848981Z"
    }
   },
   "source": [
    "# Now if you search for pandas you should see the new documents in the results.\n",
    "db.similarity_search_with_score(query=\"pandas\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Pandas is a popular open-source Python library widely used for data manipulation and analysis. It provides powerful data structures like DataFrames and Series, which allow users to handle and analyze structured data easily. ', metadata={'title': 'Pandas Analysis Book'}),\n",
       "  0.22041199),\n",
       " (Document(page_content='Figure 3.1 1\\nThe hike up to Mount Everest used to be pristine. Now it looks\\nmore like this. Whoâ s responsible? Are consumers or companies\\nresponsible, or both?\\njqpubliq â Recycling Center Pile â CC BY -SA 2.0.\\nOther companies are less concerned about conservation than they are about planned obsolescence . Planned\\nobsolescence is a deliberate ef fort by companies to make their products obsolete, or unusable, after a period of\\ntime. The goal is to improve a companyâ s sales by reducing the amount of time between the repeat purchases\\nconsumers make of products. When a software developer introduces a new version of product, it is usually\\ndesigned to be incompatible with older versions of it. For example, not all the formatting features are the same\\nin Microsoft W ord 2007 and 2010. Sometimes documents do not translate properly when opened in the newer\\nversion. Consequently , you will be more inclined to upgrade to the new version so you can open all W ord\\ndocuments you receive.\\nProducts that are disposable are another way in which firms have managed to reduce the amount of time between\\npurchases. Disposable lighters are an example. Do you know anyone today that owns a nondisposable lighter?\\nBelieve it or not, prior to the 1960s, scarcely anyone could have imagined using a cheap disposable lighter . There\\nare many more disposable products today than there were in years pastâincluding everything from bottled water\\nand individually wrapped snacks to single-use eye drops and cell phones.\\nFigure 3.1283 Principles ofMark eting', metadata={'source': 'https://storage.googleapis.com/strapi_cms_assets/principles_of_marketing_book.pdf', 'page': 91}),\n",
       "  0.50502574),\n",
       " (Document(page_content='The P olitical and L egal En vironment\\nAll or ganizations must comply with government regulations and understand the political and legal environments\\nin which they do business. Dif ferent government agencies enforce the numerous regulations that have been\\nestablished to protect both consumers and businesses. For example, the Sherman Act (1890) prohibits U.S.\\nfirms from restraining trade by creating monopolies and cartels. The regulations related to the act are enforced\\nby the Federal T rade Commission (FTC), which also regulates deceptive advertising. The U.S. Food and\\nDrug Administration (FDA) regulates the labeling of consumable products, such as food and medicine. One\\norganization that has been extremely busy is the Consumer Product Safety Commission, the group that sets\\nsafety standards for consumer products. Unsafe baby formula and toys with lead paint caused a big scare among\\nconsumers in 2008 and 2009.\\nFigure 2.7\\nThe U.S. Food and Drug Administration prohibits companies from using unacceptable levels of lead in\\ntoys and other household objects, such as utensils and furniture. Mattel voluntarily recalled Sar ge cars\\nmade in mid-2000.\\nU.S. Consumer Product Safety Commission â public domain.\\nAs we have explained, when or ganizations conduct business in multiple markets, they must understand that\\nregulations vary across countries and across states. Many states and countries have dif ferent laws that af fect\\nstrategy . For example, suppose you are opening up a new factory because you cannot keep up with the demand\\nfor your products. If you are considering opening the factory in France (perhaps because the demand in Europe\\nfor your product is strong), you need to know that it is illegal for employees in that country to work more than\\nthirty-five hours per week.2.2 Components ofthe Strategic Planning Process 36', metadata={'source': 'https://storage.googleapis.com/strapi_cms_assets/principles_of_marketing_book.pdf', 'page': 44}),\n",
       "  0.51088566),\n",
       " (Document(page_content='capitalize on opportunities and develop their competitive advantage. For example, strengths for PepsiCo are what\\nare called âmegaâ brands, or brands that individually generate over $1 billion in sales1. These brands are also\\ndesigned to contribute to PepsiCoâ s environmental and social responsibilities.\\nPepsiCoâ s brand awareness, profitability , and strong presence in global markets are also strengths. Especially in\\nforeign markets, the loyalty of a firmâ s employees can be a major strength, which can provide it with a competitive\\nadvantage. Loyal and knowledgeable employees are easier to train and tend to develop better relationships with\\ncustomers. This helps or ganizations pursue more opportunities.\\nAlthough the brand awareness for PepsiCoâ s products is strong, smaller companies often struggle with weaknesses\\nsuch as low brand awareness, low financial reserves, and poor locations. When or ganizations assess their internal\\nenvironments, they must look at factors such as performance and costs as well as brand awareness and location.\\nManagers need to examine both the past and current strategies of their firms and determine what strategies\\nsucceeded and which ones failed. This helps a company plan its future actions and improves the odds they will\\nbe successful. For example, a company might look at packaging that worked very well for a product and use the\\nsame type of packaging for new products. Firms may also look at customersâ reactions to changes in products,\\nincluding packaging, to see what works and doesnâ t work. When PepsiCo changed the packaging of major brands\\nin 2008, customers had mixed responses. T ropicana switched from the familiar orange with the straw in it to a\\nnew package and customers did not like it. As a result, T ropicana changed back to their familiar orange with a\\nstraw after spending $35 million for the new package design.\\nVideo Clip\\nTropicanaâ s Recent Ad\\n\" href=\"http://www .youtube.com/watch?v=LDnkqlnhGGI\" class=\"replaced-iframe\" data-\\nurl=\"http://www .youtube.com/watch?v=LDnkqlnhGGI\">(click to see video)\\nTropicanaâ s recent ad left out the familiar orange with a straw .\\nIndividuals are also wise to look at the strategies they have tried in the past to see which ones failed and which\\nones succeeded. Have you ever done poorly on an exam? W as it the instructor âs fault, the strategy you used to\\nstudy , or did you decide not to study? See which strategies work best for you and perhaps try the same type of\\nstrategies for future exams. If a strategy did not work, see what went wrong and change it. Doing so is similar to\\nwhat or ganizations do when they analyze their internal environments.\\nAssessing the External En vironment\\nAnalyzing the external environment involves tracking conditions in the macro and micro marketplace that,\\nalthough lar gely uncontrollable, af fect the way an or ganization does business. The macro environment includes\\neconomic factors, demographic trends, cultural and social trends, political and legal regulations, technological\\nchanges, and the price and availability of natural resources. Each factor in the macro environment is discussed\\nseparately in the next section. The micro environment includes competition, suppliers, marketing intermediaries\\n(retailers, wholesalers), the public, the company , and customers. W e focus on competition in our discussion of the\\nexternal environment in the chapter . Customers, including the public will be the focus of Chapter 3 âConsumer2.2 Components ofthe Strategic Planning Process 32', metadata={'source': 'https://storage.googleapis.com/strapi_cms_assets/principles_of_marketing_book.pdf', 'page': 40}),\n",
       "  0.5134113)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Loading and Saving the Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:07:14.917901Z",
     "start_time": "2024-10-25T08:07:14.910338Z"
    }
   },
   "source": [
    "db.save_local(\n",
    "    \"data/vectorstore\"\n",
    ")  # This creates a index.faiss and index.pkl file in the data/vectorstore directory."
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:07:15.652662Z",
     "start_time": "2024-10-25T08:07:15.583384Z"
    }
   },
   "source": [
    "new_db = FAISS.load_local(\"data/vectorstore\", embeddings)\n",
    "\n",
    "docs = new_db.similarity_search_with_score(query=\"pandas\")\n",
    "\n",
    "print(docs[0])"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and no that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m new_db \u001B[38;5;241m=\u001B[39m \u001B[43mFAISS\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_local\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata/vectorstore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m docs \u001B[38;5;241m=\u001B[39m new_db\u001B[38;5;241m.\u001B[39msimilarity_search_with_score(query\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpandas\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(docs[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_community/vectorstores/faiss.py:1078\u001B[0m, in \u001B[0;36mFAISS.load_local\u001B[0;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001B[0m\n\u001B[1;32m   1063\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Load FAISS index, docstore, and index_to_docstore_id from disk.\u001B[39;00m\n\u001B[1;32m   1064\u001B[0m \n\u001B[1;32m   1065\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1075\u001B[0m \u001B[38;5;124;03m    asynchronous: whether to use async version or not\u001B[39;00m\n\u001B[1;32m   1076\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1077\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_dangerous_deserialization:\n\u001B[0;32m-> 1078\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1079\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe de-serialization relies loading a pickle file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1080\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPickle files can be modified to deliver a malicious payload that \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1081\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresults in execution of arbitrary code on your machine.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1082\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou will need to set `allow_dangerous_deserialization` to `True` to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1083\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menable deserialization. If you do this, make sure that you \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1084\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrust the source of the data. For example, if you are loading a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1085\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile that you created, and no that no one else has modified the file, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1086\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthen this is safe to do. Do not set this to `True` if you are loading \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1087\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma file from an untrusted source (e.g., some random site on the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1088\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minternet.).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1089\u001B[0m     )\n\u001B[1;32m   1090\u001B[0m path \u001B[38;5;241m=\u001B[39m Path(folder_path)\n\u001B[1;32m   1091\u001B[0m \u001B[38;5;66;03m# load index separately since it is not picklable\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and no that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.)."
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:07:29.067352Z",
     "start_time": "2024-10-25T08:07:28.207089Z"
    }
   },
   "source": [
    "# Merging two indexes together:\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"I love data engineering\",\n",
    "        metadata={\"title\": \"Data Engineering Book\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"I love pandas\",\n",
    "        metadata={\"title\": \"Pandas Analysis Book\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "vectorstore_one = FAISS.from_documents([documents[0]], embeddings)\n",
    "vectorstore_two = FAISS.from_documents([documents[1]], embeddings)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:07:29.071008Z",
     "start_time": "2024-10-25T08:07:29.068662Z"
    }
   },
   "source": [
    "print(vectorstore_one.docstore._dict)\n",
    "print(vectorstore_two.docstore._dict)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'8608d11e-fdb2-4dc8-ae39-c7caf76f1d10': Document(page_content='I love data engineering', metadata={'title': 'Data Engineering Book'})}\n",
      "{'8da8f7b7-5faf-4898-8904-57ec6a9eaf8c': Document(page_content='I love pandas', metadata={'title': 'Pandas Analysis Book'})}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:07:29.073396Z",
     "start_time": "2024-10-25T08:07:29.071651Z"
    }
   },
   "source": [
    "vectorstore_one.merge_from(vectorstore_two)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:07:29.120512Z",
     "start_time": "2024-10-25T08:07:29.118488Z"
    }
   },
   "source": [
    "print(vectorstore_one.docstore._dict)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'8608d11e-fdb2-4dc8-ae39-c7caf76f1d10': Document(page_content='I love data engineering', metadata={'title': 'Data Engineering Book'}), '8da8f7b7-5faf-4898-8904-57ec6a9eaf8c': Document(page_content='I love pandas', metadata={'title': 'Pandas Analysis Book'})}\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Filtering and Similarity Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The capability to filter is also available in the FAISS vectorstore. However, since FAISS does not inherently support this feature, it requires manual implementation. This process entails initially retrieving more results than `k`, followed by their filtration. Document filtration can be executed based on metadata. Additionally, you have the option to determine the quantity of documents you wish to fetch prior to filtering by setting the `fetch_k` parameter during any search method invocation. To illustrate, consider the following minor example:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:07:31.492236Z",
     "start_time": "2024-10-25T08:07:29.868117Z"
    }
   },
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Sample list of data engineering documents with content and metadata\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Data engineering involves designing data pipelines.\",\n",
    "        metadata=dict(page=1),\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"ETL is a key process in data engineering workflows.\",\n",
    "        metadata=dict(page=1),\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Data modeling ensures data integrity and accessibility.\",\n",
    "        metadata=dict(page=2),\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Data warehouses are centralized repositories for structured data.\",\n",
    "        metadata=dict(page=2),\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Scalability and performance are crucial in data engineering.\",\n",
    "        metadata=dict(page=3),\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Data lakes store raw data for future processing and analysis.\",\n",
    "        metadata=dict(page=3),\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Data governance ensures data security and compliance.\",\n",
    "        metadata=dict(page=4),\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Data science and data engineering collaborate for data insights.\",\n",
    "        metadata=dict(page=4),\n",
    "    ),\n",
    "]\n",
    "\n",
    "db = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "results_with_scores = db.similarity_search_with_score(\n",
    "    \"data engineering\", k=2, fetch_k=20, filter={\"page\": 1}\n",
    ")  # Fetch top 20 results and return top 2 results with scores, filtered by page 1\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Data engineering involves designing data pipelines., Metadata: {'page': 1}, Score: 0.1363007128238678\n",
      "Content: ETL is a key process in data engineering workflows., Metadata: {'page': 1}, Score: 0.25524452328681946\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Maximal Marginal Relevance (MMR)\n",
    "\n",
    "Maximal Marginal Relevance (MMR) search is an algorithm used to diversify the results obtained from an information retrieval system. It was introduced to address the issue of redundancy in information retrieval results.\n",
    "\n",
    "In the context of generative AI models like GPT-4, an MMR search would strive to balance the trade-off between relevance (how closely the response matches the prompt) and novelty (how different each response is from the others).\n",
    "\n",
    "The MMR algorithm works by iteratively selecting the item that maximizes a certain criterion, typically a weighted combination of relevance to the query and dissimilarity to the already selected items. The main idea is to re-rank the list of items retrieved by an initial search in order to promote diversity.\n",
    "\n",
    "It's possible to use MMR whilst searching on a vector database:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:07:31.775530Z",
     "start_time": "2024-10-25T08:07:31.541307Z"
    }
   },
   "source": [
    "results = db.max_marginal_relevance_search(\"data engineering\", filter=dict(page=1))\n",
    "for doc in results:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Data engineering involves designing data pipelines., Metadata: {'page': 1}\n",
      "Content: ETL is a key process in data engineering workflows., Metadata: {'page': 1}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
