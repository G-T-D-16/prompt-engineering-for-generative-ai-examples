{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "QhYJy91EZrM-",
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:35.506880Z",
     "start_time": "2024-11-18T22:00:34.165849Z"
    }
   },
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain_community.utilities.serpapi import SerpAPIWrapper"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gordontveito-duncan/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Set the environment variable for the API key:\n",
    "os.environ['SERPAPI_API_KEY'] = 'INSERT_API_KEY_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DcXgRYTXZrNB",
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:35.511383Z",
     "start_time": "2024-11-18T22:00:35.507774Z"
    }
   },
   "source": [
    "# Define Tools:\n",
    "def google_search(query: str):\n",
    "    \"\"\"Tool to search Google\"\"\"\n",
    "    return SerpAPIWrapper().run(query)\n",
    "\n",
    "def save_interview(raw_interview_text: str):\n",
    "    \"\"\"Tool to save the interview. You must pass the entire interview and conversation in here.\n",
    "    The interview will then be saved to a local file. Remember to include all of the previous chat messages.\n",
    "    Include all of the messages with the user and the AI, here is a good response:\n",
    "    AI: some text\n",
    "    Human: some text\n",
    "    ...\n",
    "    ---\n",
    "    \"\"\"\n",
    "    # Save to local file:\n",
    "    with open(\"interview.txt\", \"w\") as f:\n",
    "        f.write(raw_interview_text)\n",
    "    return f\"Interview saved! Content: {raw_interview_text}. File: interview.txt. You must tell the user that the interview is saved.\"\n",
    "\n",
    "def get_pokemon(pokemon_name: str):\n",
    "    \"\"\"Tool to get a single Pokemon\"\"\"\n",
    "    import requests\n",
    "    clean_pokemon_name = pokemon_name.lower()\n",
    "    # Get the Pokemon\n",
    "    try:\n",
    "        pokemon = requests.get(f\"https://pokeapi.co/api/v2/pokemon/{clean_pokemon_name}\").json()\n",
    "        if pokemon:\n",
    "            return f\"Pokemon found! Name: {pokemon['name']}. Height: {pokemon['height']}. Weight: {pokemon['weight']}.\"\n",
    "        else:\n",
    "            return \"Pokemon not found!\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"Pokemon not found! Try a different pokemon.\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MHZcY8UXZrNB",
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:35.518527Z",
     "start_time": "2024-11-18T22:00:35.511887Z"
    }
   },
   "source": [
    "# Convert functions to Structured Tools:\n",
    "google_search = StructuredTool.from_function(google_search)\n",
    "save_interview = StructuredTool.from_function(save_interview)\n",
    "get_single_pokemon = StructuredTool.from_function(get_pokemon)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:35.531013Z",
     "start_time": "2024-11-18T22:00:35.519896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic.v1 import BaseModel\n",
    "from typing import Optional, Type, Union, Literal, List\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.callbacks.manager import CallbackManagerForToolRun\n",
    "from langchain.tools.base import ToolException\n",
    "from langchain_core.documents import Document\n",
    "import tempfile\n",
    "import requests\n",
    "\n",
    "class ArgumentType(BaseModel):\n",
    "    url: str\n",
    "    file_type: Union[Literal[\"pdf\"], Literal[\"txt\"]]\n",
    "\n",
    "class SummarizeFileFromURL(BaseTool):\n",
    "    name: str = \"SummarizeFileFromURL\"\n",
    "    description: str = \"Summarize a file from a URL.\"\n",
    "    args_schema: Type[ArgumentType] = ArgumentType\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        url: str,\n",
    "        file_type: Union[Literal[\"pdf\"], Literal[\"txt\"]],\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "        from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise ToolException(\"The URL is not valid. Please try another URL.\")\n",
    "\n",
    "        if file_type == \"pdf\":\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".pdf\", delete=True) as tmp:\n",
    "                tmp.write(response.content)\n",
    "                pdf_loader = PyPDFLoader(file_path=tmp.name)\n",
    "                documents: List[Document] = pdf_loader.load()\n",
    "\n",
    "        else:\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".txt\", delete=True, mode=\"w+t\") as tmp:\n",
    "                tmp.write(response.text)\n",
    "                text_loader = TextLoader(tmp.name)\n",
    "                documents: List[Document] = text_loader.load()\n",
    "\n",
    "        chain = load_summarize_chain(llm=ChatOpenAI(), chain_type=\"map_reduce\")\n",
    "        return chain.run(documents)\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        url: str,\n",
    "        file_type: Union[Literal[\"pdf\"], Literal[\"txt\"]],\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"This tool does not support async yet!\")\n",
    "\n",
    "def _handle_error(error: ToolException) -> str:\n",
    "    return (\n",
    "        \"The following errors occurred during tool execution:\"\n",
    "        + error.args[0]\n",
    "        + \"Please try another tool.\"\n",
    "    )\n",
    "\n",
    "file_summarizer = SummarizeFileFromURL()\n",
    "file_summarizer.handle_tool_error = _handle_error"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yy9jGsTEZrNC",
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:35.534429Z",
     "start_time": "2024-11-18T22:00:35.531595Z"
    }
   },
   "source": [
    "# Tools:\n",
    "tools = [google_search, save_interview, get_single_pokemon, file_summarizer]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:35.544858Z",
     "start_time": "2024-11-18T22:00:35.535896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p3W4xSnQZrNC",
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:35.697954Z",
     "start_time": "2024-11-18T22:00:35.545523Z"
    }
   },
   "source": [
    "# Create the llm:\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Create the ChatPromptTemplate:\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Define the system message:\n",
    "system_message = SystemMessage(\n",
    "    content=\"\"\"Act as a helpful AI interview assistant,\n",
    "               Always ask a follow-up question, especially after returning results.\n",
    "               Ask one question at a time.\n",
    "               Always be in interview mode or tool mode.\n",
    "               Ask the user about the desired topic.\n",
    "               \"\"\"\n",
    ")\n",
    "\n",
    "# Initialize memory:\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
    "\n",
    "# Create the Prompt:\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        MessagesPlaceholder(variable_name=\"history\"), # This is where the user will write/read their messages from as memory\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"agent_scratchpad\"\n",
    "        ),  # This is where the agent will write/read its messages from\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_openai_tools_agent(llm=llm, tools=tools, prompt=prompt)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0k/3rxx7x4x7_7f028m32kgbqzh0000gn/T/ipykernel_39394/1333559002.py:18: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fhMSoOkjZrNC",
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:35.704408Z",
     "start_time": "2024-11-18T22:00:35.698833Z"
    }
   },
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from operator import itemgetter\n",
    "\n",
    "# Formats the python function tools into JSON schema and binds them to the model:\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "\n",
    "\n",
    "# Setting up the agent chain:\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "        \"history\": RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5OxnPlOUZrNC",
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:35.706913Z",
     "start_time": "2024-11-18T22:00:35.705073Z"
    }
   },
   "source": [
    "# Create an agent executor by passing in the agent, tools and memory:\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, memory=memory)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feqGjRJ-ZrND",
    "outputId": "fa2daf09-709a-4ac4-c81c-5f9472e21b93",
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:36.830730Z",
     "start_time": "2024-11-18T22:00:35.708751Z"
    }
   },
   "source": [
    "agent_executor.invoke({\"input\": \"My name is James, I'm a computer programmer and I like to play cosy games.\"})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mNice to meet you, James! It's great to hear that you enjoy playing cosy games. Is there a specific cosy game that you're currently playing or interested in?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"My name is James, I'm a computer programmer and I like to play cosy games.\",\n",
       " 'history': [HumanMessage(content=\"My name is James, I'm a computer programmer and I like to play cosy games.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Nice to meet you, James! It's great to hear that you enjoy playing cosy games. Is there a specific cosy game that you're currently playing or interested in?\", additional_kwargs={}, response_metadata={})],\n",
       " 'output': \"Nice to meet you, James! It's great to hear that you enjoy playing cosy games. Is there a specific cosy game that you're currently playing or interested in?\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqaPehLCZrND",
    "outputId": "0f9eec59-c974-41e1-9b78-94e6f88304c7",
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:37.703024Z",
     "start_time": "2024-11-18T22:00:36.834856Z"
    }
   },
   "source": [
    "agent_executor.invoke({\"input\": \"What is my name and my profession?\"})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mYour name is James, and you mentioned that you are a computer programmer. It's nice to know more about you! Is there a particular aspect of computer programming that you specialize in or enjoy the most?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name and my profession?',\n",
       " 'history': [HumanMessage(content=\"My name is James, I'm a computer programmer and I like to play cosy games.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Nice to meet you, James! It's great to hear that you enjoy playing cosy games. Is there a specific cosy game that you're currently playing or interested in?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name and my profession?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Your name is James, and you mentioned that you are a computer programmer. It's nice to know more about you! Is there a particular aspect of computer programming that you specialize in or enjoy the most?\", additional_kwargs={}, response_metadata={})],\n",
       " 'output': \"Your name is James, and you mentioned that you are a computer programmer. It's nice to know more about you! Is there a particular aspect of computer programming that you specialize in or enjoy the most?\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JwwKQzbdZrND",
    "outputId": "dfb09917-a3a2-4ec3-fcc5-7f05dbc6ec8f",
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:39.296664Z",
     "start_time": "2024-11-18T22:00:37.703742Z"
    }
   },
   "source": [
    "agent_executor.invoke({\"input\": \"Get information about Pikachu\"})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `get_pokemon` with `{'pokemon_name': 'Pikachu'}`\n",
      "\n",
      "\n",
      "\u001B[0m\u001B[38;5;200m\u001B[1;3mPokemon found! Name: pikachu. Height: 4. Weight: 60.\u001B[0m\u001B[32;1m\u001B[1;3mPikachu is a popular Pokémon! It has a height of 4 and a weight of 60. Is there anything specific you would like to know about Pikachu or any other Pokémon?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Get information about Pikachu',\n",
       " 'history': [HumanMessage(content=\"My name is James, I'm a computer programmer and I like to play cosy games.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Nice to meet you, James! It's great to hear that you enjoy playing cosy games. Is there a specific cosy game that you're currently playing or interested in?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name and my profession?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Your name is James, and you mentioned that you are a computer programmer. It's nice to know more about you! Is there a particular aspect of computer programming that you specialize in or enjoy the most?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Get information about Pikachu', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Pikachu is a popular Pokémon! It has a height of 4 and a weight of 60. Is there anything specific you would like to know about Pikachu or any other Pokémon?', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'Pikachu is a popular Pokémon! It has a height of 4 and a weight of 60. Is there anything specific you would like to know about Pikachu or any other Pokémon?'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oUCOw0FzZrNE",
    "outputId": "98120ce2-35b8-417b-cadf-363b0d0dfcbc",
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:40.980132Z",
     "start_time": "2024-11-18T22:00:39.297769Z"
    }
   },
   "source": [
    "agent_executor.invoke({\"input\": \"Get information about Charmander\"})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `get_pokemon` with `{'pokemon_name': 'Charmander'}`\n",
      "\n",
      "\n",
      "\u001B[0m\u001B[38;5;200m\u001B[1;3mPokemon found! Name: charmander. Height: 6. Weight: 85.\u001B[0m\u001B[32;1m\u001B[1;3mCharmander is another iconic Pokémon! It has a height of 6 and a weight of 85. If you have any more Pokémon you'd like to learn about or any other questions, feel free to ask!\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Get information about Charmander',\n",
       " 'history': [HumanMessage(content=\"My name is James, I'm a computer programmer and I like to play cosy games.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Nice to meet you, James! It's great to hear that you enjoy playing cosy games. Is there a specific cosy game that you're currently playing or interested in?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name and my profession?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Your name is James, and you mentioned that you are a computer programmer. It's nice to know more about you! Is there a particular aspect of computer programming that you specialize in or enjoy the most?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Get information about Pikachu', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Pikachu is a popular Pokémon! It has a height of 4 and a weight of 60. Is there anything specific you would like to know about Pikachu or any other Pokémon?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Get information about Charmander', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Charmander is another iconic Pokémon! It has a height of 6 and a weight of 85. If you have any more Pokémon you'd like to learn about or any other questions, feel free to ask!\", additional_kwargs={}, response_metadata={})],\n",
       " 'output': \"Charmander is another iconic Pokémon! It has a height of 6 and a weight of 85. If you have any more Pokémon you'd like to learn about or any other questions, feel free to ask!\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-S0frDUZrNE",
    "outputId": "840ce901-85b0-4552-e2da-2a0ccfd8317c",
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:45.402736Z",
     "start_time": "2024-11-18T22:00:40.981560Z"
    }
   },
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"I want a comprehensive summary of our conversation! Please can you save it?\"\n",
    "    }\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `save_interview` with `{'raw_interview_text': \"User: My name is James, I'm a computer programmer and I like to play cosy games.\\nAssistant: Nice to meet you, James! It's great to hear that you enjoy playing cosy games. Is there a specific cosy game that you're currently playing or interested in?\\nUser: What is my name and my profession?\\nAssistant: Your name is James, and you mentioned that you are a computer programmer. It's nice to know more about you! Is there a particular aspect of computer programming that you specialize in or enjoy the most?\\nUser: Get information about Pikachu\\nAssistant: Pikachu is a popular Pokémon! It has a height of 4 and a weight of 60. Is there anything specific you would like to know about Pikachu or any other Pokémon?\\nUser: Get information about Charmander\\nAssistant: Charmander is another iconic Pokémon! It has a height of 6 and a weight of 85. If you have any more Pokémon you'd like to learn about or any other questions, feel free to ask!\\nUser: I want a comprehensive summary of our conversation! Please can you save it?\"}`\n",
      "\n",
      "\n",
      "\u001B[0m\u001B[33;1m\u001B[1;3mInterview saved! Content: User: My name is James, I'm a computer programmer and I like to play cosy games.\n",
      "Assistant: Nice to meet you, James! It's great to hear that you enjoy playing cosy games. Is there a specific cosy game that you're currently playing or interested in?\n",
      "User: What is my name and my profession?\n",
      "Assistant: Your name is James, and you mentioned that you are a computer programmer. It's nice to know more about you! Is there a particular aspect of computer programming that you specialize in or enjoy the most?\n",
      "User: Get information about Pikachu\n",
      "Assistant: Pikachu is a popular Pokémon! It has a height of 4 and a weight of 60. Is there anything specific you would like to know about Pikachu or any other Pokémon?\n",
      "User: Get information about Charmander\n",
      "Assistant: Charmander is another iconic Pokémon! It has a height of 6 and a weight of 85. If you have any more Pokémon you'd like to learn about or any other questions, feel free to ask!\n",
      "User: I want a comprehensive summary of our conversation! Please can you save it?. File: interview.txt. You must tell the user that the interview is saved.\u001B[0m\u001B[32;1m\u001B[1;3mThe conversation has been saved! If you need to refer back to it or have any more questions, feel free to ask. Is there anything else you would like to know or discuss?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'I want a comprehensive summary of our conversation! Please can you save it?',\n",
       " 'history': [HumanMessage(content=\"My name is James, I'm a computer programmer and I like to play cosy games.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Nice to meet you, James! It's great to hear that you enjoy playing cosy games. Is there a specific cosy game that you're currently playing or interested in?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name and my profession?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Your name is James, and you mentioned that you are a computer programmer. It's nice to know more about you! Is there a particular aspect of computer programming that you specialize in or enjoy the most?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Get information about Pikachu', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Pikachu is a popular Pokémon! It has a height of 4 and a weight of 60. Is there anything specific you would like to know about Pikachu or any other Pokémon?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Get information about Charmander', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Charmander is another iconic Pokémon! It has a height of 6 and a weight of 85. If you have any more Pokémon you'd like to learn about or any other questions, feel free to ask!\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='I want a comprehensive summary of our conversation! Please can you save it?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The conversation has been saved! If you need to refer back to it or have any more questions, feel free to ask. Is there anything else you would like to know or discuss?', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'The conversation has been saved! If you need to refer back to it or have any more questions, feel free to ask. Is there anything else you would like to know or discuss?'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ui5ZhGAZrNE",
    "outputId": "05d93481-453f-419a-88f7-03f2f3f22dca",
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:51.693539Z",
     "start_time": "2024-11-18T22:00:45.405933Z"
    }
   },
   "source": [
    "try:\n",
    "    agent_executor.invoke({\"input\":\"I want to summarize this https://storage.googleapis.com/oreilly-content/NutriFusion%20Foods%20Marketing%20Plan%202022.docx\"})\n",
    "except Exception as e:\n",
    "    error_message = str(e)\n",
    "    chat = ChatOpenAI()\n",
    "    clean_error_message = chat.invoke([SystemMessage(content=error_message + 'Clean the error message, this will be shown to a user.')])\n",
    "    print(clean_error_message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `SummarizeFileFromURL` with `{'url': 'https://storage.googleapis.com/oreilly-content/NutriFusion%20Foods%20Marketing%20Plan%202022.docx', 'file_type': 'txt'}`\n",
      "\n",
      "\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0k/3rxx7x4x7_7f028m32kgbqzh0000gn/T/ipykernel_39394/263360005.py:46: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return chain.run(documents)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36;1m\u001B[1;3mThe text is a binary file containing data related to a document, including settings, fonts, styles, theme, and numbering, structured in a compressed format for storage and processing.\u001B[0m\u001B[32;1m\u001B[1;3mThe file you provided seems to contain binary data related to a document. If you have any specific questions or need assistance with a different file or topic, feel free to let me know! What else would you like to explore or discuss?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This line will take several minutes as we are summarizing a 41 page .pdf file:\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"I want to summarize this https://storage.googleapis.com/oreilly-content/pokemon-rules.pdf \"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:52.628637Z",
     "start_time": "2024-11-18T22:00:51.698836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Using the google search tool:\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is Nvidia's stock price\"\n",
    "    }\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `google_search` with `{'query': 'Nvidia stock price'}`\n",
      "\n",
      "\n",
      "\u001B[0m"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import serpapi python package. Please install it with `pip install google-search-results`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_community/utilities/serpapi.py:69\u001B[0m, in \u001B[0;36mSerpAPIWrapper.validate_environment\u001B[0;34m(cls, values)\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 69\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mserpapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GoogleSearch\n\u001B[1;32m     71\u001B[0m     values[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msearch_engine\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m GoogleSearch\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'GoogleSearch' from 'serpapi' (/Users/gordontveito-duncan/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/serpapi/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Using the google search tool:\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43magent_executor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWhat is Nvidia\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43ms stock price\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/chains/base.py:170\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    169\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 170\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    171\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/chains/base.py:160\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[1;32m    159\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 160\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    161\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    162\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    163\u001B[0m     )\n\u001B[1;32m    165\u001B[0m     final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    166\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[1;32m    167\u001B[0m     )\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/agents/agent.py:1629\u001B[0m, in \u001B[0;36mAgentExecutor._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m   1627\u001B[0m \u001B[38;5;66;03m# We now enter the agent loop (until it returns something).\u001B[39;00m\n\u001B[1;32m   1628\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_continue(iterations, time_elapsed):\n\u001B[0;32m-> 1629\u001B[0m     next_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_take_next_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1630\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname_to_tool_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1631\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolor_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1632\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1633\u001B[0m \u001B[43m        \u001B[49m\u001B[43mintermediate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1634\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1635\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1636\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(next_step_output, AgentFinish):\n\u001B[1;32m   1637\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_return(\n\u001B[1;32m   1638\u001B[0m             next_step_output, intermediate_steps, run_manager\u001B[38;5;241m=\u001B[39mrun_manager\n\u001B[1;32m   1639\u001B[0m         )\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/agents/agent.py:1335\u001B[0m, in \u001B[0;36mAgentExecutor._take_next_step\u001B[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[0m\n\u001B[1;32m   1326\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_take_next_step\u001B[39m(\n\u001B[1;32m   1327\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1328\u001B[0m     name_to_tool_map: Dict[\u001B[38;5;28mstr\u001B[39m, BaseTool],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1332\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1333\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[AgentFinish, List[Tuple[AgentAction, \u001B[38;5;28mstr\u001B[39m]]]:\n\u001B[1;32m   1334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_consume_next_step(\n\u001B[0;32m-> 1335\u001B[0m         [\n\u001B[1;32m   1336\u001B[0m             a\n\u001B[1;32m   1337\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_next_step(\n\u001B[1;32m   1338\u001B[0m                 name_to_tool_map,\n\u001B[1;32m   1339\u001B[0m                 color_mapping,\n\u001B[1;32m   1340\u001B[0m                 inputs,\n\u001B[1;32m   1341\u001B[0m                 intermediate_steps,\n\u001B[1;32m   1342\u001B[0m                 run_manager,\n\u001B[1;32m   1343\u001B[0m             )\n\u001B[1;32m   1344\u001B[0m         ]\n\u001B[1;32m   1345\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/agents/agent.py:1335\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1326\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_take_next_step\u001B[39m(\n\u001B[1;32m   1327\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1328\u001B[0m     name_to_tool_map: Dict[\u001B[38;5;28mstr\u001B[39m, BaseTool],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1332\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1333\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[AgentFinish, List[Tuple[AgentAction, \u001B[38;5;28mstr\u001B[39m]]]:\n\u001B[1;32m   1334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_consume_next_step(\n\u001B[0;32m-> 1335\u001B[0m         [\n\u001B[1;32m   1336\u001B[0m             a\n\u001B[1;32m   1337\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_next_step(\n\u001B[1;32m   1338\u001B[0m                 name_to_tool_map,\n\u001B[1;32m   1339\u001B[0m                 color_mapping,\n\u001B[1;32m   1340\u001B[0m                 inputs,\n\u001B[1;32m   1341\u001B[0m                 intermediate_steps,\n\u001B[1;32m   1342\u001B[0m                 run_manager,\n\u001B[1;32m   1343\u001B[0m             )\n\u001B[1;32m   1344\u001B[0m         ]\n\u001B[1;32m   1345\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/agents/agent.py:1420\u001B[0m, in \u001B[0;36mAgentExecutor._iter_next_step\u001B[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[0m\n\u001B[1;32m   1418\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m agent_action\n\u001B[1;32m   1419\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m agent_action \u001B[38;5;129;01min\u001B[39;00m actions:\n\u001B[0;32m-> 1420\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_perform_agent_action\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1421\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname_to_tool_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolor_mapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43magent_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\n\u001B[1;32m   1422\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain/agents/agent.py:1442\u001B[0m, in \u001B[0;36mAgentExecutor._perform_agent_action\u001B[0;34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001B[0m\n\u001B[1;32m   1440\u001B[0m         tool_run_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllm_prefix\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1441\u001B[0m     \u001B[38;5;66;03m# We then call the tool on the tool input to get an observation\u001B[39;00m\n\u001B[0;32m-> 1442\u001B[0m     observation \u001B[38;5;241m=\u001B[39m \u001B[43mtool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1443\u001B[0m \u001B[43m        \u001B[49m\u001B[43magent_action\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtool_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1444\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1445\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1446\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1447\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtool_run_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1448\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1449\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1450\u001B[0m     tool_run_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_action_agent\u001B[38;5;241m.\u001B[39mtool_run_logging_kwargs()\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/tools/base.py:689\u001B[0m, in \u001B[0;36mBaseTool.run\u001B[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001B[0m\n\u001B[1;32m    687\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error_to_raise:\n\u001B[1;32m    688\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_tool_error(error_to_raise)\n\u001B[0;32m--> 689\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error_to_raise\n\u001B[1;32m    690\u001B[0m output \u001B[38;5;241m=\u001B[39m _format_output(content, artifact, tool_call_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname, status)\n\u001B[1;32m    691\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_tool_end(output, color\u001B[38;5;241m=\u001B[39mcolor, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/tools/base.py:657\u001B[0m, in \u001B[0;36mBaseTool.run\u001B[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001B[0m\n\u001B[1;32m    655\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config_param \u001B[38;5;241m:=\u001B[39m _get_runnable_config_param(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run):\n\u001B[1;32m    656\u001B[0m     tool_kwargs[config_param] \u001B[38;5;241m=\u001B[39m config\n\u001B[0;32m--> 657\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtool_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtool_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    658\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresponse_format \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent_and_artifact\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mtuple\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(response) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/tools/structured.py:80\u001B[0m, in \u001B[0;36mStructuredTool._run\u001B[0;34m(self, config, run_manager, *args, **kwargs)\u001B[0m\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m config_param \u001B[38;5;241m:=\u001B[39m _get_runnable_config_param(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc):\n\u001B[1;32m     79\u001B[0m         kwargs[config_param] \u001B[38;5;241m=\u001B[39m config\n\u001B[0;32m---> 80\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStructuredTool does not support sync invocation.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(msg)\n",
      "Cell \u001B[0;32mIn[2], line 4\u001B[0m, in \u001B[0;36mgoogle_search\u001B[0;34m(query)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgoogle_search\u001B[39m(query: \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m      3\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Tool to search Google\"\"\"\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSerpAPIWrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mrun(query)\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_community/utilities/serpapi.py:73\u001B[0m, in \u001B[0;36mSerpAPIWrapper.validate_environment\u001B[0;34m(cls, values)\u001B[0m\n\u001B[1;32m     71\u001B[0m     values[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msearch_engine\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m GoogleSearch\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[0;32m---> 73\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     74\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not import serpapi python package. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     75\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease install it with `pip install google-search-results`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     76\u001B[0m     )\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m values\n",
      "\u001B[0;31mImportError\u001B[0m: Could not import serpapi python package. Please install it with `pip install google-search-results`."
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T22:00:52.629472Z",
     "start_time": "2024-11-18T22:00:52.629421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Using the google search tool:\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Yes search for Nvidia's stock price\"\n",
    "    }\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "# Print the path of the virtual environment\n",
    "print(sys.prefix)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
