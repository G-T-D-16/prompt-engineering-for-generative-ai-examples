{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first part of an LCEL chain must be a runnable type."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:37:58.502782Z",
     "start_time": "2024-10-12T08:37:58.444969Z"
    }
   },
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "bad_first_input = {\n",
    "    \"film_required_age\": 18,\n",
    "}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate a film title, the age is {film_required_age}\"\n",
    ")\n",
    "\n",
    "# This will error:\n",
    "bad_chain = bad_first_input | prompt"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 14\u001B[0m\n\u001B[1;32m      9\u001B[0m prompt \u001B[38;5;241m=\u001B[39m ChatPromptTemplate\u001B[38;5;241m.\u001B[39mfrom_template(\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerate a film title, the age is \u001B[39m\u001B[38;5;132;01m{film_required_age}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     11\u001B[0m )\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# This will error:\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m bad_chain \u001B[38;5;241m=\u001B[39m \u001B[43mbad_first_input\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m|\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:434\u001B[0m, in \u001B[0;36mRunnable.__ror__\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m    424\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__ror__\u001B[39m(\n\u001B[1;32m    425\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    426\u001B[0m     other: Union[\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    431\u001B[0m     ],\n\u001B[1;32m    432\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RunnableSerializable[Other, Output]:\n\u001B[1;32m    433\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compose this runnable with another object to create a RunnableSequence.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 434\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m RunnableSequence(\u001B[43mcoerce_to_runnable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:4967\u001B[0m, in \u001B[0;36mcoerce_to_runnable\u001B[0;34m(thing)\u001B[0m\n\u001B[1;32m   4965\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m RunnableLambda(cast(Callable[[Input], Output], thing))\n\u001B[1;32m   4966\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(thing, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m-> 4967\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(Runnable[Input, Output], \u001B[43mRunnableParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthing\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   4968\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   4969\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m   4970\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected a Runnable, callable or dict.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4971\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInstead got an unsupported type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(thing)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4972\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:3007\u001B[0m, in \u001B[0;36mRunnableParallel.__init__\u001B[0;34m(self, steps__, **kwargs)\u001B[0m\n\u001B[1;32m   3004\u001B[0m merged \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msteps__} \u001B[38;5;28;01mif\u001B[39;00m steps__ \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m   3005\u001B[0m merged\u001B[38;5;241m.\u001B[39mupdate(kwargs)\n\u001B[1;32m   3006\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m-> 3007\u001B[0m     steps__\u001B[38;5;241m=\u001B[39m{key: coerce_to_runnable(r) \u001B[38;5;28;01mfor\u001B[39;00m key, r \u001B[38;5;129;01min\u001B[39;00m merged\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[1;32m   3008\u001B[0m )\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:3007\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   3004\u001B[0m merged \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msteps__} \u001B[38;5;28;01mif\u001B[39;00m steps__ \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m   3005\u001B[0m merged\u001B[38;5;241m.\u001B[39mupdate(kwargs)\n\u001B[1;32m   3006\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m-> 3007\u001B[0m     steps__\u001B[38;5;241m=\u001B[39m{key: \u001B[43mcoerce_to_runnable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mr\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m key, r \u001B[38;5;129;01min\u001B[39;00m merged\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[1;32m   3008\u001B[0m )\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:4969\u001B[0m, in \u001B[0;36mcoerce_to_runnable\u001B[0;34m(thing)\u001B[0m\n\u001B[1;32m   4967\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001B[1;32m   4968\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 4969\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m   4970\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected a Runnable, callable or dict.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4971\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInstead got an unsupported type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(thing)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4972\u001B[0m     )\n",
      "\u001B[0;31mTypeError\u001B[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'int'>"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:38:01.643011Z",
     "start_time": "2024-10-12T08:38:01.523786Z"
    }
   },
   "source": [
    "# All of these chains enforce the runnable interface:\n",
    "first_good_input = {\"film_required_age\": itemgetter(\"film_required_age\")}\n",
    "\n",
    "# Creating a dictionary within a RunnableLambda:\n",
    "second_good_input = RunnableLambda(lambda x: { \"film_required_age\": x[\"film_required_age\"] } )\n",
    "\n",
    "third_good_input = RunnablePassthrough()\n",
    "fourth_good_input = {\"film_required_age\": RunnablePassthrough()}\n",
    "# You can also create a chain starting with RunnableParallel\n",
    "\n",
    "first_good_chain = first_good_input | prompt\n",
    "second_good_chain = second_good_input | prompt\n",
    "third_good_chain = third_good_input | prompt\n",
    "fourth_good_chain = fourth_good_input | prompt\n",
    "\n",
    "first_good_chain.invoke({\n",
    "    \"film_required_age\": 18\n",
    "})\n",
    "\n",
    "# ...\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Generate a film title, the age is 18')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order Matters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:39:05.422669Z",
     "start_time": "2024-10-12T08:39:05.182548Z"
    }
   },
   "source": [
    "bad_order_chain = prompt | first_good_input\n",
    "bad_order_chain.invoke({\"film_required_age\": 18})"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ChatPromptValue' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m bad_order_chain \u001B[38;5;241m=\u001B[39m prompt \u001B[38;5;241m|\u001B[39m first_good_input\n\u001B[0;32m----> 2\u001B[0m \u001B[43mbad_order_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfilm_required_age\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m18\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:2499\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config)\u001B[0m\n\u001B[1;32m   2497\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   2498\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps):\n\u001B[0;32m-> 2499\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2500\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2501\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# mark each step as a child run\u001B[39;49;00m\n\u001B[1;32m   2502\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpatch_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2503\u001B[0m \u001B[43m                \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseq:step:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2504\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2505\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2506\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   2507\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:3144\u001B[0m, in \u001B[0;36mRunnableParallel.invoke\u001B[0;34m(self, input, config)\u001B[0m\n\u001B[1;32m   3131\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[1;32m   3132\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   3133\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(\n\u001B[1;32m   3134\u001B[0m                 step\u001B[38;5;241m.\u001B[39minvoke,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3142\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m   3143\u001B[0m         ]\n\u001B[0;32m-> 3144\u001B[0m         output \u001B[38;5;241m=\u001B[39m {key: future\u001B[38;5;241m.\u001B[39mresult() \u001B[38;5;28;01mfor\u001B[39;00m key, future \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(steps, futures)}\n\u001B[1;32m   3145\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   3146\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:3144\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   3131\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[1;32m   3132\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   3133\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(\n\u001B[1;32m   3134\u001B[0m                 step\u001B[38;5;241m.\u001B[39minvoke,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3142\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m   3143\u001B[0m         ]\n\u001B[0;32m-> 3144\u001B[0m         output \u001B[38;5;241m=\u001B[39m {key: \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m key, future \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(steps, futures)}\n\u001B[1;32m   3145\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   3146\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:438\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 438\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_condition\u001B[38;5;241m.\u001B[39mwait(timeout)\n\u001B[1;32m    442\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:390\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 390\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    391\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    392\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    393\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/thread.py:52\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:3967\u001B[0m, in \u001B[0;36mRunnableLambda.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   3965\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001B[39;00m\n\u001B[1;32m   3966\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 3967\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_with_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3968\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_invoke\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3969\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3970\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3971\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3972\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3973\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3974\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m   3975\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke a coroutine function synchronously.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3976\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUse `ainvoke` instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3977\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:1625\u001B[0m, in \u001B[0;36mRunnable._call_with_config\u001B[0;34m(self, func, input, config, run_type, **kwargs)\u001B[0m\n\u001B[1;32m   1621\u001B[0m     context \u001B[38;5;241m=\u001B[39m copy_context()\n\u001B[1;32m   1622\u001B[0m     context\u001B[38;5;241m.\u001B[39mrun(var_child_runnable_config\u001B[38;5;241m.\u001B[39mset, child_config)\n\u001B[1;32m   1623\u001B[0m     output \u001B[38;5;241m=\u001B[39m cast(\n\u001B[1;32m   1624\u001B[0m         Output,\n\u001B[0;32m-> 1625\u001B[0m         \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1626\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcall_func_with_variable_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1627\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1628\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1629\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1630\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1631\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1632\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m   1633\u001B[0m     )\n\u001B[1;32m   1634\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1635\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/runnables/config.py:347\u001B[0m, in \u001B[0;36mcall_func_with_variable_args\u001B[0;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[1;32m    346\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m run_manager\n\u001B[0;32m--> 347\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:3841\u001B[0m, in \u001B[0;36mRunnableLambda._invoke\u001B[0;34m(self, input, run_manager, config, **kwargs)\u001B[0m\n\u001B[1;32m   3839\u001B[0m                 output \u001B[38;5;241m=\u001B[39m chunk\n\u001B[1;32m   3840\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3841\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mcall_func_with_variable_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3842\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m   3843\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3844\u001B[0m \u001B[38;5;66;03m# If the output is a runnable, invoke it\u001B[39;00m\n\u001B[1;32m   3845\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, Runnable):\n",
      "File \u001B[0;32m~/Documents/GitHub/prompt-engineering-for-generative-ai-examples/venv/lib/python3.9/site-packages/langchain_core/runnables/config.py:347\u001B[0m, in \u001B[0;36mcall_func_with_variable_args\u001B[0;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[1;32m    346\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m run_manager\n\u001B[0;32m--> 347\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'ChatPromptValue' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that your story generation will require multiple sequential prompts you can use a create `SequentialChain` to chain multiple prompts together. The `SequentialChain` will need to re-use the outputs of the previous prompt and use it as the input for the next prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:39:14.942664Z",
     "start_time": "2024-10-12T08:39:14.935637Z"
    }
   },
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:39:22.968200Z",
     "start_time": "2024-10-12T08:39:22.964137Z"
    }
   },
   "source": [
    "character_generation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"I want you to brainstorm 3 - 5 characters for my short story. The genre is {genre}.\n",
    "    Each character must have a Name and a Biography.\n",
    "    You must provide a name and biography for each character, this is very important!\n",
    "    ---\n",
    "    Example response:\n",
    "    Name: CharWiz, Biography: A wizard who is a master of magic.\n",
    "    Name: CharWar, Biography: A warrior who is a master of the sword.\n",
    "    ---\n",
    "    Characters: \"\"\"\n",
    ")\n",
    "\n",
    "plot_generation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Given the following characters and the genre, create an effective plot for a short story:\n",
    "    Characters:\n",
    "    {characters} \n",
    "    ---\n",
    "    Genre: {genre}\n",
    "    ---\n",
    "    Plot: \"\"\"\n",
    "    )\n",
    "\n",
    "scene_generation_plot_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Act as an effective content creator. \n",
    "    Given multiple characters and a plot you are responsible generating the various scenes for each act. \n",
    "    \n",
    "    You must de-compose the plot into multiple effective scenes:\n",
    "\n",
    "    ---\n",
    "    Characters:\n",
    "    {characters}\n",
    "    ---\n",
    "    Genre: {genre}\n",
    "    ---\n",
    "    Plot: {plot}\n",
    "    ---\n",
    "    Example response:\n",
    "    Scenes:\n",
    "    Scene 1: Some text here.\n",
    "    Scene 2: Some text here.\n",
    "    Scene 3: Some text here.\n",
    "    ----\n",
    "    Scenes:\n",
    "    \"\"\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ItemGetter and RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:39:25.684179Z",
     "start_time": "2024-10-12T08:39:25.679911Z"
    }
   },
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from operator import itemgetter"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:39:26.432474Z",
     "start_time": "2024-10-12T08:39:26.421660Z"
    }
   },
   "source": [
    "chain = RunnablePassthrough() | {\n",
    "    \"genre\": itemgetter(\"genre\"),\n",
    "}\n",
    "chain.invoke({\"genre\": \"fantasy\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genre': 'fantasy'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:39:31.215196Z",
     "start_time": "2024-10-12T08:39:31.204257Z"
    }
   },
   "source": [
    "# Another way to achieve the same thing:\n",
    "chain = RunnableLambda(lambda x: {\"genre\": x[\"genre\"]}) | {\"genre\": itemgetter(\"genre\")}\n",
    "chain.invoke({\"genre\": \"fantasy\"})\n",
    "# {'genre': 'fantasy'}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genre': 'fantasy'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:39:32.949816Z",
     "start_time": "2024-10-12T08:39:32.938643Z"
    }
   },
   "source": [
    "chain = RunnablePassthrough() | {\n",
    "    \"genre\": itemgetter(\"genre\"),\n",
    "    \"upper_case_genre\": lambda x: x[\"genre\"].upper(),\n",
    "    \"lower_case_genre\": RunnableLambda(lambda x: x[\"genre\"].lower()),\n",
    "}\n",
    "chain.invoke({\"genre\": \"fantasy\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genre': 'fantasy',\n",
       " 'upper_case_genre': 'FANTASY',\n",
       " 'lower_case_genre': 'fantasy'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:39:40.909865Z",
     "start_time": "2024-10-12T08:39:40.879170Z"
    }
   },
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "master_chain = RunnablePassthrough() | {\n",
    "    \"genre\": itemgetter(\"genre\"),\n",
    "    \"upper_case_genre\": lambda x: x[\"genre\"].upper(),\n",
    "    \"lower_case_genre\": RunnableLambda(lambda x: x[\"genre\"].lower()),   \n",
    "}\n",
    "\n",
    "master_chain_two = RunnablePassthrough() | RunnableParallel(\n",
    "        genre=itemgetter(\"genre\"),\n",
    "        upper_case_genre=lambda x: x[\"genre\"].upper(),\n",
    "        lower_case_genre=RunnableLambda(lambda x: x[\"genre\"].lower()),\n",
    ")\n",
    "\n",
    "story_result = master_chain.invoke({\"genre\": \"Fantasy\"})\n",
    "print(f\"master chain: {story_result}\")\n",
    "\n",
    "story_result = master_chain_two.invoke({\"genre\": \"Fantasy\"})\n",
    "print(f\"master chain two: {story_result}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master chain: {'genre': 'Fantasy', 'upper_case_genre': 'FANTASY', 'lower_case_genre': 'fantasy'}\n",
      "master chain two: {'genre': 'Fantasy', 'upper_case_genre': 'FANTASY', 'lower_case_genre': 'fantasy'}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:40:22.726998Z",
     "start_time": "2024-10-12T08:40:22.713220Z"
    }
   },
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:40:23.364324Z",
     "start_time": "2024-10-12T08:40:23.334599Z"
    }
   },
   "source": [
    "# Create the chat model:\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Create the sub-chains:\n",
    "character_generation_chain = character_generation_prompt | model |  StrOutputParser()\n",
    "plot_generation_chain = plot_generation_prompt | model | StrOutputParser()                                                               \n",
    "scene_generation_plot_chain = scene_generation_plot_prompt | model | StrOutputParser()"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:40:38.324769Z",
     "start_time": "2024-10-12T08:40:27.932676Z"
    }
   },
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "master_chain = (\n",
    "    {\"characters\": character_generation_chain, \"genre\": RunnablePassthrough()}\n",
    "    | RunnableParallel(\n",
    "        characters=itemgetter(\"characters\"),\n",
    "        genre=itemgetter(\"genre\"),\n",
    "        plot=plot_generation_chain,\n",
    "    )\n",
    "    | RunnableParallel(\n",
    "        characters=itemgetter(\"characters\"),\n",
    "        genre=itemgetter(\"genre\"),\n",
    "        plot=itemgetter(\"plot\"),\n",
    "        scenes=scene_generation_plot_chain,\n",
    "    )\n",
    ")\n",
    "\n",
    "story_result = master_chain.invoke({\"genre\": \"Fantasy\"})"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:40:40.171831Z",
     "start_time": "2024-10-12T08:40:40.169270Z"
    }
   },
   "source": [
    "print(story_result['scenes'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene 1: Elara practices her magical abilities in a secluded clearing, determined to perfect her skills and prove herself to her family and the world.\n",
      "\n",
      "Scene 2: Thorn sneaks through the shadows of a bustling marketplace, using his quick wit and agility to avoid detection as he gathers information on the dark sorcerer's whereabouts.\n",
      "\n",
      "Scene 3: Seraphina trains with her legendary sword, honing her skills and preparing for the battle ahead as she vows to protect the innocent and uphold justice in the face of darkness.\n",
      "\n",
      "Scene 4: Orion roams the forests, his bow at the ready as he communicates with the animals and senses the growing threat of the dark sorcerer, determined to protect the land from any harm.\n",
      "\n",
      "Scene 5: The group comes together, forming a powerful team as they embark on their journey to confront the dark sorcerer, facing treacherous obstacles and mythical creatures along the way.\n",
      "\n",
      "Scene 6: Lyra's hidden tower comes into view, shrouded in mystery and magic, as she emerges to offer her assistance to the group with her powerful spells and potions.\n",
      "\n",
      "Scene 7: The final battle ensues, with Elara, Thorn, Seraphina, Orion, and Lyra combining their strengths and abilities to face off against the dark sorcerer in an epic showdown of magic and might.\n",
      "\n",
      "Scene 8: As the dust settles and the dark sorcerer is defeated, the group emerges victorious, hailed as legendary heroes in the realm of fantasy for their bravery, teamwork, and determination to protect the land from evil.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Story Scene Generation:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:41:44.505161Z",
     "start_time": "2024-10-12T08:41:44.496379Z"
    }
   },
   "source": [
    "# Extracting the scenes using .split('\\n') and removing empty strings:\n",
    "scenes = [scene for scene in story_result[\"scenes\"].split(\"\\n\") if scene]\n",
    "generated_scenes = []\n",
    "previous_scene_summary = \"\""
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:41:45.654189Z",
     "start_time": "2024-10-12T08:41:45.650685Z"
    }
   },
   "source": [
    "character_script_prompt = ChatPromptTemplate.from_template(\n",
    "    template=\"\"\"Given the following characters: {characters} and the genre: {genre}, create an effective character script for a scene.\n",
    "\n",
    "    You must follow the following principles:\n",
    "    - Use the Previous Scene Summary: {previous_scene_summary} to avoid repeating yourself.\n",
    "    - Use the Plot: {plot} to create an effective scene character script.\n",
    "    - Currently you are generating the character dialogue script for the following scene: {scene}\n",
    "\n",
    "    ---\n",
    "    Here is an example response:\n",
    "    SCENE 1: ANNA'S APARTMENT\n",
    "\n",
    "    (ANNA is sorting through old books when there is a knock at the door. She opens it to reveal JOHN.)\n",
    "    ANNA: Can I help you, sir?\n",
    "    JOHN: Perhaps, I think it's me who can help you. I heard you're researching time travel.\n",
    "    (Anna looks intrigued but also cautious.)\n",
    "    ANNA: That's right, but how do you know?\n",
    "    JOHN: You could say... I'm a primary source.\n",
    "\n",
    "    ---\n",
    "    SCENE NUMBER: {index}\n",
    "\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "summarize_prompt = ChatPromptTemplate.from_template(\n",
    "    template=\"\"\"Given a character script create a summary of the scene. Character script: {character_script}\"\"\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:42:13.865033Z",
     "start_time": "2024-10-12T08:41:48.878197Z"
    }
   },
   "source": [
    "# Loading a chat model:\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo-16k')\n",
    "\n",
    "# Create the LCEL chains:\n",
    "character_script_generation_chain = (\n",
    "    {\n",
    "        \"characters\": RunnablePassthrough(),\n",
    "        \"genre\": RunnablePassthrough(),\n",
    "        \"previous_scene_summary\": RunnablePassthrough(),\n",
    "        \"plot\": RunnablePassthrough(),\n",
    "        \"scene\": RunnablePassthrough(),\n",
    "        \"index\": RunnablePassthrough(),\n",
    "    }\n",
    "    | character_script_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summarize_chain = summarize_prompt | model | StrOutputParser()\n",
    "\n",
    "# You might want to use tqdm here to track the progress, or use all of the scenes:\n",
    "for index, scene in enumerate(scenes[0:5]):\n",
    "    \n",
    "    # # Create a scene generation:\n",
    "    scene_result = character_script_generation_chain.invoke(\n",
    "        {\n",
    "            \"characters\": story_result[\"characters\"],\n",
    "            \"genre\": \"fantasy\",\n",
    "            \"previous_scene_summary\": previous_scene_summary,\n",
    "            \"index\": index,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Store the generated scenes:\n",
    "    generated_scenes.append(\n",
    "        {\"character_script\": scene_result, \"scene\": scenes[index]}\n",
    "    )\n",
    "\n",
    "    # If this is the first scene then we don't have a previous scene summary:\n",
    "    if index == 0:\n",
    "        previous_scene_summary = scene_result\n",
    "    else:\n",
    "        # If this is the second scene or greater then we can use and generate a summary:\n",
    "        summary_result = summarize_chain.invoke(\n",
    "            {\"character_script\": scene_result}\n",
    "        )\n",
    "        previous_scene_summary = summary_result"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:44:44.302138Z",
     "start_time": "2024-10-12T08:44:44.297804Z"
    }
   },
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:44:45.196495Z",
     "start_time": "2024-10-12T08:44:45.184351Z"
    }
   },
   "source": [
    "df = pd.DataFrame(generated_scenes)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:44:51.377792Z",
     "start_time": "2024-10-12T08:44:51.361062Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                    character_script  \\\n",
       "0  SCENE 1: THE ANCIENT FOREST\\n\\n(ELARA, THORN, ...   \n",
       "1  SCENE 2: THE ANCIENT TEMPLE\\n\\n(ELARA, THORN, ...   \n",
       "2  SCENE 2: ENTRANCE OF THE ANCIENT TEMPLE\\n\\n(Th...   \n",
       "3  SCENE 3: ANCIENT TEMPLE ENTRANCE\\n\\n(The group...   \n",
       "4  SCENE 4: ANCIENT TEMPLE ENTRANCE\\n\\n(The group...   \n",
       "\n",
       "                                               scene  \n",
       "0  Scene 1: Elara practices her magical abilities...  \n",
       "1  Scene 2: Thorn sneaks through the shadows of a...  \n",
       "2  Scene 3: Seraphina trains with her legendary s...  \n",
       "3  Scene 4: Orion roams the forests, his bow at t...  \n",
       "4  Scene 5: The group comes together, forming a p...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character_script</th>\n",
       "      <th>scene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCENE 1: THE ANCIENT FOREST\\n\\n(ELARA, THORN, ...</td>\n",
       "      <td>Scene 1: Elara practices her magical abilities...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCENE 2: THE ANCIENT TEMPLE\\n\\n(ELARA, THORN, ...</td>\n",
       "      <td>Scene 2: Thorn sneaks through the shadows of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SCENE 2: ENTRANCE OF THE ANCIENT TEMPLE\\n\\n(Th...</td>\n",
       "      <td>Scene 3: Seraphina trains with her legendary s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SCENE 3: ANCIENT TEMPLE ENTRANCE\\n\\n(The group...</td>\n",
       "      <td>Scene 4: Orion roams the forests, his bow at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SCENE 4: ANCIENT TEMPLE ENTRANCE\\n\\n(The group...</td>\n",
       "      <td>Scene 5: The group comes together, forming a p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:46:20.387188Z",
     "start_time": "2024-10-12T08:46:20.377255Z"
    }
   },
   "cell_type": "code",
   "source": "df.shape",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:46:22.483529Z",
     "start_time": "2024-10-12T08:46:22.473408Z"
    }
   },
   "source": [
    "all_character_script_text = \"\\n\".join(df.character_script.tolist())\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1500, chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs = text_splitter.create_documents([all_character_script_text])"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T08:46:26.331819Z",
     "start_time": "2024-10-12T08:46:23.419968Z"
    }
   },
   "source": [
    "chain = load_summarize_chain(llm=model, chain_type=\"map_reduce\")\n",
    "summary = chain.invoke(docs)\n",
    "print(summary['output_text'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A group of adventurers gather in an ancient forest to prepare for a dangerous quest to retrieve a powerful artifact. They discuss potential dangers and obstacles, remaining unified and determined. They stand before the entrance of an ancient temple, sensing lurking dangers and a powerful presence. Despite their trepidation, they push open the doors and enter with excitement.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
